# Описание алгоритма

Нам дан ориентированный ациклический граф (DAG). Для каждой вершины нужно посчитать значение как некоторую функцию от значений вершин, в которые из данной направлены рёбра. Из одной вершины может исходить несколько рёбер, в одну вершину может входить несколько рёбер. 

Идея алгоритма проста -- обойти граф в порядке топологической сортировки и посчитать значение для каждой вершины. Такой порядок обхода гарантирует, что когда мы собираемся вычислить значение очередной вершины, значения вершин, от которых она зависит, уже вычислены. 

Шаги алгоритма:
1. Снабдим каждую вершину счётчиком `wait_for_dependecies_count` - это количество инцидентных вершин, для которых нужно посчитать значение перед тем, как начать считать значение для данной вершины. 
2. Будем складывать очередь `wait_for_process` вершины, для которых уже можно вычислить значение. 
3. Так как граф ациклический, то гарантированно есть вершины, у которых уже задано значение. Переберём все такие вершины. Для очередной вершины `V` пройдём по всем входящим в неё рёбрам и уменьшим `wait_for_dependecies_count` на 1 для достигнутых вершин (другими словами, для каждого ребра `U -> V` мы делаем `U.wait_for_dependecies_count -= 1`). Если для какой-то вершины `wait_for_dependecies_count` стало равно нулю, значит, для этой вершины можно вычислить значение, и мы помещаем её в очередь `wait_for_process`.
4. Теперь запускаем цикл: пока в очереди `wait_for_process` есть элементы:
   1. Достаём из неё очередную вершину
   2. Считаем её значение, суммируя значения по всем исходящим рёбрам
   3. Проходим по всем входящим рёбрам и делаем то же самое, что и на шаге 3. 
5. Если очередь опустела, значит, значения для всех вершин посчитаны    

# Ход работы

## Описание общих деталей реализации

Вершина графа представлена классом `Node`, который лежит в файле [graph.h](./graph.h). Она хранит два списка указателей на вершины:
* `dependencies_` -- это список вершин, в которые из данной исходят рёбра
* `dependent_` -- это список вершин, из которых рёбра входят в данную вершину

Сам граф мы представляем как `std::deque<Node>`. Выбор `std::deque` тут важен, потому что `push_back` в него не инвалидирует указатели на его элементы (а нам важна валидность указателей, так как в `Node` мы храним указатели на другие вершины).

## Шаг 1 - однопоточное решение

Я начал с создания однопоточного решения, чтобы у меня был baseline, относительно которого я буду искать более быстрое решение.

Детали реализации:
* [Однопоточное решение](./single_thread.cpp) использует в качестве очереди обычную `std::queue`.
* Счётчик `wait_for_dependecies_count` был простым `int`'ом, но потом был заменён на `std::atomic<int>`, чтобы использовать один и тот же код класса `Node` и в многопоточной реализации, и в однопоточной.
* Для подсчёта значения вершины используется функция `int64_t CalculateNodeValue(const Node& cur)` в файле [graph.cpp](./graph.cpp).

Работа велась на ноутбуке с такими характеристиками:
* CPU Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz, 12 cores
* 32 GB RAM
* CPU Caches:
  * L1 Data 32 KiB (x6)
  * L1 Instruction 32 KiB (x6)
  * L2 Unified 256 KiB (x6)
  * L3 Unified 12288 KiB (x1)

Обработка тестового примера однопоточным решением заняло 2,3 сек:

```
$ time cmake-build-release/spreadsheet st ~/Downloads/input.txt /dev/null

real	0m2,268s
user	0m2,224s
sys	0m0,044s
```

При этом профилирование показало, что бОльшую часть времени программа тратит на чтение графа. Значит, нужно создавать отдельные бенчмарки, чтобы замерять, сколько времени работает именно обработка графа. 
```
   - 95,10% main
      + 78,35% ReadGraph
      + 9,03% CalculateValuesST
      + 6,73% std::deque<Node, std::allocator<Node> >::~deque
        0,54% std::__ostream_insert<char, std::char_traits<char> >              
```

## Шаг 2 -- лобовое многопоточное решение

Первый шаг к параллелизации решения - разбирать очередь `wait_for_process` не одним, а в несколько потоков. Я написал самую лобовую реализацию многопоточной очереди, чтобы не делать преждевременной оптимизации -- лучше пусть потом профилировщик покажет, где на самом деле узкое место. 

Детали [первой версии многопоточной реализации](./multi_thread_one.cpp):
* заводим атомарный счётчик `nodes_left` -- это количество вершин, для которых ещё нужно посчитать значение
* очередь - это просто `std::queue` под мьютексом
* создаём столько тредов, сколько возвращает `std::thread::hardware_concurrency()`
* каждый поток выполняет активное ожидание: 
   * пока `nodes_left > 0`, он пытается достать что-то из очереди `wait_for_process`
   * если не смог, пробует снова
   * если смог, он вычисляет значение в этой вершине, уменьшает `nodes_left` на один и добавляет в очередь те вершины, для которых теперь нужно посчитать значение. 
   
Обработка графа из примера таким многопоточным алгоритмом не дала видимого ускорения -- те же 2,3 секунды.
```
$ time cmake-build-release/spreadsheet mt ~/Downloads/input.txt /dev/null

real	0m2,317s
user	0m2,748s
sys	0m1,459s
```

Значит, пора делать бенчмарки

## Шаг 3 -- бенчмарки

Я решил использовать библиотеку Google Benchmark. Созданные бенчмарки лежат в файле [benchmark.cpp](./benchmark.cpp). Я решил проверять сравнивать скорость работы однопоточной и многопоточной реализации на графах разных размеров. Для этого мне нужно было научиться генерировать графы. Я сделал это по следующему алгоритму:
1. Задаём число рёбер как число вершин, умноженное на случайное целое число от 2 до 10 - таким образом, значения большинства вершин надо будет вычислять и лишь для малой доли вершин оно будет задано заранее
2. Для каждого ребра выбираем два случайных индекса и проводим ребро из вершины с меньшим индексом в вершину с большим - так мы гарантируем, что у нас не будет циклов. 

Для бенчмарков строим графы размером 100, 10 000 и 1 000 000 вершин. Последний используем, потому что во входном примере ~400 000 вершин, а в условии сказано, что тестировать решение будут на гораздо большем числе вершин.

Результаты первых бенчмарков:

```
--------------------------------------------------------------------
Benchmark                          Time             CPU   Iterations
--------------------------------------------------------------------
BM_SingleThread/100             1878 ns         1878 ns       365166
BM_SingleThread/10000         462438 ns       462412 ns         1592
BM_SingleThread/1000000    467379671 ns    467335524 ns            2
BM_MultiThreadOne/100         134245 ns       126392 ns         4782
BM_MultiThreadOne/10000      3172903 ns       454085 ns         1000
BM_MultiThreadOne/1000000  477720254 ns     65989520 ns           10
```

Как и предсказывалось в условии, если просто суммировать значения, многопоточное решение не даёт никакого выигрыша по времени, а на маленьких размерах графов ещё и работает медленнее, чем однопоточное. Поэтому добавляем в функцию суммирования задержку <= 4 мкс:

```c++
int64_t CalculateNodeValue(const Node& cur) {
  std::chrono::microseconds delay{std::hash<std::string>()(cur.Name()) % 5};
  std::this_thread::sleep_for(delay);
  ...
}  
```

Тут важно, что в качестве значения задержки используется хеш от имени вершины - это сделано, чтобы для одной и той же вершины значение задержки было фиксировано. Так правильнее сравнивать между собой время работы разных реализаций.

Запускаем бенчмарки:
```
--------------------------------------------------------------------
Benchmark                          Time             CPU   Iterations
--------------------------------------------------------------------
BM_SingleThread/100           3075643 ns        61667 ns         1000
BM_SingleThread/10000       326684177 ns      6823492 ns           10
BM_SingleThread/1000000   39256600473 ns   1249976104 ns            1
BM_MultiThreadOne/100         1225298 ns       268965 ns         2606
BM_MultiThreadOne/10000      27541057 ns       472398 ns          100
BM_MultiThreadOne/1000000  3332736255 ns     57170263 ns            1
```

Отлично! Многопоточная реализация обогнала однопоточную. Теперь будем искать в ней узкие места и оптимизировать. 

## Как проводить измерения

```bash
$ for i in st mt mtb; do echo $i; time cmake-build-release/spreadsheet $i ~/Downloads/input.txt /dev/null; done
st

real	0m27,982s
user	0m2,852s
sys	0m0,210s
mt
Thread count is 12

real	0m4,290s
user	0m3,084s
sys	0m0,208s
```

## Текущий статус

MultiThreadTwoBatches работает за то же время, что и MultiThreadOne. При этом в его профиле я не вижу, чтобы треды страдали от contention - 72% времени они занимаются подсчётом значения вершины.  